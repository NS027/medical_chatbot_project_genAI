{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec86cc1",
   "metadata": {},
   "source": [
    "# Develop of model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8759c68",
   "metadata": {},
   "source": [
    "## Stage 1\n",
    "\n",
    "### Dataset Preparation\n",
    "Download the orginal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3715099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-19.0.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: packaging in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from aiohttp->datasets) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/chris/Desktop/Study/Term5/Final Project/medical_chat_bot/7180_project/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached pyarrow-19.0.1-cp311-cp311-macosx_12_0_arm64.whl (30.7 MB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
      "Successfully installed datasets-3.5.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-19.0.1 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8694b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0ee0f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 414k/414k [00:00<00:00, 7.62MB/s]\n",
      "Downloading data: 100%|██████████| 57.7k/57.7k [00:00<00:00, 13.2MB/s]\n",
      "Downloading data: 100%|██████████| 52.0k/52.0k [00:00<00:00, 14.0MB/s]\n",
      "Generating train split: 100%|██████████| 482/482 [00:00<00:00, 34522.20 examples/s]\n",
      "Generating validation split: 100%|██████████| 60/60 [00:00<00:00, 16233.92 examples/s]\n",
      "Generating test split: 100%|██████████| 61/61 [00:00<00:00, 26070.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(path=\"UCSD26/medical_dialog\", name='processed.en',trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83485c1",
   "metadata": {},
   "source": [
    "check the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c0da52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['description', 'utterances'],\n",
      "        num_rows: 482\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['description', 'utterances'],\n",
      "        num_rows: 60\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['description', 'utterances'],\n",
      "        num_rows: 61\n",
      "    })\n",
      "})\n",
      "\n",
      "Splits available: dict_keys(['train', 'validation', 'test'])\n",
      "\n",
      "Number of examples in train split: 482\n",
      "\n",
      "Number of examples in validation split: 60\n",
      "\n",
      "Number of examples in test split: 61\n",
      "\n",
      "Features: {'description': Value(dtype='string', id=None), 'utterances': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n",
      "\n",
      "Example dialogues from the train split:\n",
      "\n",
      "Example 1:\n",
      "description: throat a bit sore and want to get a good imune booster, especially in light of the virus. please adv...\n",
      "utterances: ['patient: throat a bit sore and want to get a good imune booster, especially in light of the virus. please advise. have not been in contact with nyone with the virus.', \"doctor: during this pandemic. throat pain can be from a strep throat infection (antibiotics needed), a cold or influenza or other virus, or from some other cause such as allergies or irritants. usually, a person sees the doctor (call first) if the sore throat is bothersome, recurrent, or doesn't go away quickly. covid-19 infections tend to have cough, whereas strep throat usually lacks cough but has more throat pain. (3/21/20)\"]\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "description: hey there i have had cold \"symptoms\" for over a week and had a low grade fever last week. for the pa...\n",
      "utterances: ['patient: hey there i have had cold \"symptoms\" for over a week and had a low grade fever last week. for the past two days i have been feeling dizzy. should i contact my dr? should i see a dr', 'doctor: yes. protection. it is not enough symptoms to say that you are a suspect case of covid19; but, independently of this, if you have been in contact with a case, or you present persistent cough (with or without sputum), shortness of breath, wheezing, or you have a chronic disease like diabetes, hypertension, low immune system or cancer, should ask for medical attention. and use all the protection measures.']\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "description: i have a tight and painful chest with a dry cough, no fever and no headaches. could it possibly be c...\n",
      "utterances: ['patient: i have a tight and painful chest with a dry cough, no fever and no headaches. could it possibly be coronavirus?', 'doctor: possible. top symptoms include fever, dry cough and sob. an obvious possibility. if so, your best step is to self-quarntine. remember at your age low risk of complication and typically will pass without issue. if worsening sob be seen. call your provider or check with local health department. these are healthtap guidelines:https://www.healthtap.com/blog/covid-19-care-guidelines/self-quarantine-guide.']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"Dataset structure:\", dataset)\n",
    "print(\"\\nSplits available:\", dataset.keys())\n",
    "\n",
    "# Get the number of examples in each split\n",
    "for split in dataset.keys():\n",
    "    print(f\"\\nNumber of examples in {split} split:\", len(dataset[split]))\n",
    "\n",
    "# Look at the features/columns in the dataset\n",
    "print(\"\\nFeatures:\", dataset[next(iter(dataset.keys()))].features)\n",
    "\n",
    "# Display a few examples from the training set (or main split)\n",
    "main_split = next(iter(dataset.keys()))\n",
    "print(f\"\\nExample dialogues from the {main_split} split:\")\n",
    "for i in range(3):  # Show first 3 examples\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    example = dataset[main_split][i]\n",
    "    for key, value in example.items():\n",
    "        if isinstance(value, str) and len(value) > 100:\n",
    "            print(f\"{key}: {value[:100]}...\")  # Truncate long text\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde279ab",
   "metadata": {},
   "source": [
    "Create three jsonl file for the dataset.\n",
    "- train\n",
    "- test\n",
    "- validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eff9682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec4d4afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: stage1_train.jsonl, stage1_validation.jsonl, stage1_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset with all splits\n",
    "dataset = load_dataset(path=\"UCSD26/medical_dialog\", name='processed.en',trust_remote_code=True)\n",
    "\n",
    "# Format function for one split\n",
    "def format_split(split_data):\n",
    "    formatted = []\n",
    "    for item in split_data:\n",
    "        utterances = item['utterances']\n",
    "        if len(utterances) >= 2:\n",
    "            patient = next((u.split(\"patient:\")[1].strip() for u in utterances if u.lower().startswith(\"patient:\")), None)\n",
    "            doctor = next((u.split(\"doctor:\")[1].strip() for u in utterances if u.lower().startswith(\"doctor:\")), None)\n",
    "            if patient and doctor:\n",
    "                formatted.append({\n",
    "                    \"prompt\": f\"### Instruction:\\n{patient}\\n\\n### Response:\",\n",
    "                    \"response\": doctor\n",
    "                })\n",
    "    return formatted\n",
    "\n",
    "# Format each split\n",
    "train_data = format_split(dataset[\"train\"])\n",
    "val_data = format_split(dataset[\"validation\"])\n",
    "test_data = format_split(dataset[\"test\"])\n",
    "\n",
    "# Save to JSONL\n",
    "def save_jsonl(data, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for entry in data:\n",
    "            json.dump(entry, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "# Save files to your custom path\n",
    "save_jsonl(train_data, \"datasets/stage1_data/stage1_train.jsonl\")\n",
    "save_jsonl(val_data, \"datasets/stage1_data/stage1_validation.jsonl\")\n",
    "save_jsonl(test_data, \"datasets/stage1_data/stage1_test.jsonl\")\n",
    "\n",
    "print(\"✅ Saved: stage1_train.jsonl, stage1_validation.jsonl, stage1_test.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172a279f",
   "metadata": {},
   "source": [
    "Unsloth uses the ChatML format (like OpenAI’s messages format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf7ae3",
   "metadata": {},
   "source": [
    "### Upload to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20ee8848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 482 examples [00:00, 83158.02 examples/s]\n",
      "Generating train split: 60 examples [00:00, 21245.95 examples/s]\n",
      "Generating train split: 61 examples [00:00, 32497.47 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 482/482 [00:00<00:00, 145390.47 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 60/60 [00:00<00:00, 24847.77 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 61/61 [00:00<00:00, 26136.74 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to disk at datasets/stage1_data/medical_dialog_dataset_unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load from local files\n",
    "train = load_dataset(\"json\", data_files=\"datasets/stage1_data_chatml/stage1_train.jsonl\")[\"train\"]\n",
    "val = load_dataset(\"json\", data_files=\"datasets/stage1_data_chatml/stage1_validation.jsonl\")[\"train\"]\n",
    "test = load_dataset(\"json\", data_files=\"datasets/stage1_data_chatml/stage1_test.jsonl\")[\"train\"]\n",
    "\n",
    "# Combine into DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train,\n",
    "    \"validation\": val,\n",
    "    \"test\": test\n",
    "})\n",
    "\n",
    "# Save the dataset to disk\n",
    "dataset.save_to_disk(\"datasets/stage1_data/medical_dialog_dataset_unsloth\")\n",
    "\n",
    "print(\"Dataset saved to disk at datasets/stage1_data/medical_dialog_dataset_unsloth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3cac0c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train split to datasets/medical_dialog_chatml/train.txt with 482 examples\n",
      "Saved validation split to datasets/medical_dialog_chatml/validation.txt with 60 examples\n",
      "Saved test split to datasets/medical_dialog_chatml/test.txt with 61 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 482/482 [00:00<00:00, 163273.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 60/60 [00:00<00:00, 21126.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 61/61 [00:00<00:00, 27728.68 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved complete dataset to medical_dialog_chatml_hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Load the original dataset\n",
    "dataset = load_dataset(path=\"UCSD26/medical_dialog\", name=\"processed.en\", trust_remote_code=True)\n",
    "\n",
    "def convert_to_chatml(example):\n",
    "    \"\"\"Convert a conversation to ChatML format\"\"\"\n",
    "    \n",
    "    utterances = example[\"utterances\"]\n",
    "    chatml_text = \"<|im_start|>system\\nYou are a helpful medical assistant that provides accurate and ethical medical information.<|im_end|>\\n\"\n",
    "    \n",
    "    for utterance in utterances:\n",
    "        # Determine if this is a patient (user) or doctor (assistant) message\n",
    "        if utterance.lower().startswith(\"patient:\"):\n",
    "            role = \"user\"\n",
    "            # Remove the \"patient:\" prefix and trim\n",
    "            content = re.sub(r'^patient:\\s*', '', utterance, flags=re.IGNORECASE).strip()\n",
    "        elif utterance.lower().startswith(\"doctor:\"):\n",
    "            role = \"assistant\"\n",
    "            # Remove the \"doctor:\" prefix and trim\n",
    "            content = re.sub(r'^doctor:\\s*', '', utterance, flags=re.IGNORECASE).strip()\n",
    "        else:\n",
    "            # If no clear prefix, try to determine based on position\n",
    "            # In most datasets, odd indices are user, even are assistant\n",
    "            continue  # Skip if can't determine role\n",
    "        \n",
    "        chatml_text += f\"<|im_start|>{role}\\n{content}<|im_end|>\\n\"\n",
    "    \n",
    "    return {\"text\": chatml_text}\n",
    "\n",
    "# Convert each split\n",
    "chatml_datasets = {}\n",
    "for split in dataset.keys():\n",
    "    chatml_datasets[split] = dataset[split].map(convert_to_chatml)\n",
    "\n",
    "# Create a new DatasetDict with the converted data\n",
    "chatml_dataset = DatasetDict(chatml_datasets)\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(\"datasets/medical_dialog_chatml\", exist_ok=True)\n",
    "\n",
    "# Save each split separately\n",
    "for split in chatml_dataset.keys():\n",
    "    # Save each split as a separate file\n",
    "    output_file = os.path.join(\"datasets/medical_dialog_chatml\", f\"{split}.txt\")\n",
    "    \n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for example in chatml_dataset[split]:\n",
    "            f.write(example[\"text\"])\n",
    "            f.write(\"\\n\\n\")  # Add blank lines between conversations\n",
    "    \n",
    "    print(f\"Saved {split} split to {output_file} with {len(chatml_dataset[split])} examples\")\n",
    "\n",
    "# Also save the entire dataset in HF format for later use\n",
    "chatml_dataset.save_to_disk(\"datasets/medical_dialog_chatml_hf\")\n",
    "print(\"Saved complete dataset to medical_dialog_chatml_hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4256d47",
   "metadata": {},
   "source": [
    "### Stage 2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81ed4191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 250/250 [00:00<00:00, 84067.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 25/25 [00:00<00:00, 10561.80 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 25/25 [00:00<00:00, 10446.06 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to disk at datasets/dental_implant_straumann\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load from your labeled JSONL files\n",
    "dataset = DatasetDict({\n",
    "    \"train\": load_dataset(\"json\", data_files=\"datasets/stage2_data/straumann_stage2_train_labeled.jsonl\", split=\"train\"),\n",
    "    \"validation\": load_dataset(\"json\", data_files=\"datasets/stage2_data/straumann_stage2_validation_labeled.jsonl\", split=\"train\"),\n",
    "    \"test\": load_dataset(\"json\", data_files=\"datasets/stage2_data/straumann_stage2_test_labeled.jsonl\", split=\"train\"),\n",
    "})\n",
    "\n",
    "# Save it back to the same folder (or a subfolder to avoid overwriting)\n",
    "dataset.save_to_disk(\"datasets/stage2_data/dental_implant_straumann\")\n",
    "print(\"Dataset saved to disk at datasets/dental_implant_straumann\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088fa011",
   "metadata": {},
   "source": [
    "### Combine two dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32c61874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged 732 entries into 'datasets/stage3_data/merged_train.jsonl'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# --- Load Stage 1 ---\n",
    "with open(\"datasets/stage1_data/stage1_train.jsonl\", \"r\") as f1:\n",
    "    stage1 = [json.loads(line) for line in f1]\n",
    "\n",
    "# Convert to unified format\n",
    "stage1_converted = [\n",
    "    {\n",
    "        \"instruction\": item[\"prompt\"].replace(\"### Instruction:\\n\", \"\").replace(\"### Response:\", \"\").strip(),\n",
    "        \"response\": item[\"response\"].strip()\n",
    "    }\n",
    "    for item in stage1\n",
    "]\n",
    "\n",
    "# --- Load Stage 2 ---\n",
    "with open(\"datasets/stage2_data/straumann_stage2_train.jsonl\", \"r\") as f2:\n",
    "    stage2 = [json.loads(line) for line in f2]\n",
    "\n",
    "# Convert to unified format\n",
    "stage2_converted = []\n",
    "for item in stage2:\n",
    "    if \"text\" in item:\n",
    "        parts = item[\"text\"].split(\"### Response:\\n\")\n",
    "        if len(parts) == 2:\n",
    "            instruction = parts[0].replace(\"### Instruction:\\n\", \"\").strip()\n",
    "            response = parts[1].strip()\n",
    "            stage2_converted.append({\n",
    "                \"instruction\": instruction,\n",
    "                \"response\": response\n",
    "            })\n",
    "\n",
    "# --- Merge ---\n",
    "merged = stage1_converted + stage2_converted\n",
    "\n",
    "# --- Save ---\n",
    "with open(\"datasets/stage3_data/merged_train.jsonl\", \"w\") as f_out:\n",
    "    for item in merged:\n",
    "        json.dump(item, f_out)\n",
    "        f_out.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Merged {len(merged)} entries into 'datasets/stage3_data/merged_train.jsonl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ca3be23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 86 examples [00:00, 28958.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 732/732 [00:00<00:00, 236371.59 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 85/85 [00:00<00:00, 27913.86 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 86/86 [00:00<00:00, 29371.40 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to disk at atasets/stage3_data/doctor_chat_dental_implants_qa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load from your labeled JSONL files\n",
    "dataset = DatasetDict({\n",
    "    \"train\": load_dataset(\"json\", data_files=\"datasets/stage3_data/merged_train.jsonl\", split=\"train\"),\n",
    "    \"validation\": load_dataset(\"json\", data_files=\"datasets/stage3_data/merged_validation.jsonl\", split=\"train\"),\n",
    "    \"test\": load_dataset(\"json\", data_files=\"datasets/stage3_data/merged_test.jsonl\", split=\"train\"),\n",
    "})\n",
    "\n",
    "# Save it back to the same folder (or a subfolder to avoid overwriting)\n",
    "dataset.save_to_disk(\"datasets/stage3_data/doctor_chat_dental_implants_qa\")\n",
    "print(\"Dataset saved to disk at atasets/stage3_data/doctor_chat_dental_implants_qa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f94874",
   "metadata": {},
   "source": [
    "### Change the style to match the unsloth alpaca style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72758c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 732/732 [00:00<00:00, 63408.31 examples/s]\n",
      "Map: 100%|██████████| 85/85 [00:00<00:00, 28439.36 examples/s]\n",
      "Map: 100%|██████████| 86/86 [00:00<00:00, 24501.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 732/732 [00:00<00:00, 315218.74 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 85/85 [00:00<00:00, 44497.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 86/86 [00:00<00:00, 41087.84 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to disk: datasets/stage3_data/doctor_dental_alpaca_format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load tokenizer and eos token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# Load your dataset\n",
    "dataset = load_from_disk(\"datasets/stage3_data/doctor_chat_dental_implants_qa\")\n",
    "\n",
    "# Define Alpaca format\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# Format function\n",
    "def formatting_prompts_func(examples):\n",
    "    return {\n",
    "        \"text\": [\n",
    "            alpaca_prompt.format(instr, resp) + EOS_TOKEN\n",
    "            for instr, resp in zip(examples[\"instruction\"], examples[\"response\"])\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Apply formatting\n",
    "formatted_dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Save to disk\n",
    "formatted_dataset.save_to_disk(\"datasets/stage3_data/doctor_dental_alpaca_format\")\n",
    "print(\"✅ Saved to disk: datasets/stage3_data/doctor_dental_alpaca_format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ffb62a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nthroat a bit sore and want to get a good imune booster, especially in light of the virus. please advise. have not been in contact with nyone with the virus.\\n\\n### Response:\\nduring this pandemic. throat pain can be from a strep throat infection (antibiotics needed), a cold or influenza or other virus, or from some other cause such as allergies or irritants. usually, a person sees the doctor (call first) if the sore throat is bothersome, recurrent, or doesn't go away quickly. covid-19 infections tend to have cough, whereas strep throat usually lacks cough but has more throat pain. (3/21/20)<|im_end|>\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_dataset[\"train\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "112bb550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<|im_end|>'\n"
     ]
    }
   ],
   "source": [
    "print(repr(tokenizer.eos_token))  # should show '</s>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7fdf2e",
   "metadata": {},
   "source": [
    "### Convert Alpaca-style JSONL to Llama 3.2 chat-style JSONL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f7396",
   "metadata": {},
   "source": [
    "stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c76cd8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. Saved to datasets/stage1_data/stage1_converted_validation.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Input/output paths\n",
    "input_file = \"datasets/stage1_data/stage1_validation.jsonl\"\n",
    "output_file = \"datasets/stage1_data/stage1_converted_validation.jsonl\"\n",
    "\n",
    "def convert_line(line):\n",
    "    # Parse the JSON line\n",
    "    data = json.loads(line)\n",
    "    \n",
    "    # Extract prompt and response\n",
    "    prompt = data.get(\"prompt\", \"\").replace(\"### Instruction:\\n\", \"\").strip()\n",
    "    response = data.get(\"response\", \"\").replace(\"### Response:\", \"\").strip()\n",
    "    \n",
    "    # Convert to the required format\n",
    "    return {\n",
    "        \"conversation\": [\n",
    "            { \"from\": \"patient\", \"value\": prompt },\n",
    "            { \"from\": \"doctor\", \"value\": response }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Process the file\n",
    "with open(input_file, 'r', encoding='utf-8') as fin, open(output_file, 'w', encoding='utf-8') as fout:\n",
    "    for line in fin:\n",
    "        converted = convert_line(line)\n",
    "        fout.write(json.dumps(converted, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Conversion complete. Saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7ba58",
   "metadata": {},
   "source": [
    "stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9d07e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. Output saved to datasets/stage2_data/straumann_converted_train.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Input and output paths\n",
    "input_file = 'datasets/stage2_data/straumann_stage2_train.jsonl'\n",
    "output_file = 'datasets/stage2_data/straumann_converted_train.jsonl'\n",
    "\n",
    "def convert_entry(entry):\n",
    "    text = entry.get(\"text\", \"\")\n",
    "    \n",
    "    # Extract the instruction and response using regex\n",
    "    instruction_match = re.search(r\"### Instruction:\\s*(.*?)\\s*### Response:\", text, re.DOTALL)\n",
    "    response_match = re.search(r\"### Response:\\s*(.*)\", text, re.DOTALL)\n",
    "\n",
    "    if not instruction_match or not response_match:\n",
    "        return None  # Skip malformed entries\n",
    "    \n",
    "    instruction = instruction_match.group(1).strip()\n",
    "    response = response_match.group(1).strip()\n",
    "    \n",
    "    return {\n",
    "        \"conversation\": [\n",
    "            { \"from\": \"human\", \"value\": instruction },\n",
    "            { \"from\": \"expert\", \"value\": response }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Process and write output\n",
    "with open(input_file, 'r', encoding='utf-8') as fin, open(output_file, 'w', encoding='utf-8') as fout:\n",
    "    for line in fin:\n",
    "        data = json.loads(line)\n",
    "        converted = convert_entry(data)\n",
    "        if converted:\n",
    "            fout.write(json.dumps(converted, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Conversion complete. Output saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a638f397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files combined and saved to datasets/stage3_data/llama_merged_train.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Input file paths\n",
    "file1 = 'datasets/stage1_data/stage1_converted_train.jsonl'\n",
    "file2 = 'datasets/stage2_data/straumann_converted_train.jsonl'\n",
    "\n",
    "# Output file path\n",
    "output_file = 'datasets/stage3_data/llama_merged_train.jsonl'\n",
    "\n",
    "# Merge them\n",
    "with open(output_file, 'w', encoding='utf-8') as fout:\n",
    "    for fname in [file1, file2]:\n",
    "        with open(fname, 'r', encoding='utf-8') as fin:\n",
    "            for line in fin:\n",
    "                fout.write(line)\n",
    "\n",
    "print(f\"Files combined and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1457bca5",
   "metadata": {},
   "source": [
    "create dataset for llama finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e2d3e6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 732 examples [00:00, 124684.48 examples/s]\n",
      "Generating train split: 85 examples [00:00, 41363.94 examples/s]\n",
      "Generating train split: 86 examples [00:00, 29646.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 732/732 [00:00<00:00, 183417.80 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 85/85 [00:00<00:00, 27951.07 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 86/86 [00:00<00:00, 28362.18 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to disk at\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load from your labeled JSONL files\n",
    "dataset = DatasetDict({\n",
    "    \"train\": load_dataset(\"json\", data_files=\"datasets/stage3_data/llama_merged_train.jsonl\", split=\"train\"),\n",
    "    \"validation\": load_dataset(\"json\", data_files=\"datasets/stage3_data/llama_merged_validation.jsonl\", split=\"train\"),\n",
    "    \"test\": load_dataset(\"json\", data_files=\"datasets/stage3_data/llama_merged_test.jsonl\", split=\"train\"),\n",
    "})\n",
    "\n",
    "# Save it back to the same folder (or a subfolder to avoid overwriting)\n",
    "dataset.save_to_disk(\"datasets/stage3_data/doctor_dental_llama_qa_new\")\n",
    "print(\"Dataset saved to disk at\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f34f7e",
   "metadata": {},
   "source": [
    "### Make own dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6b70fb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 246678/246678 [00:00<00:00, 374453.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Malikeh1375/medical-question-answering-datasets\", \"all-processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f34fd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 246678/246678 [00:00<00:00, 247901.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# save to disk\n",
    "ds.save_to_disk(\"datasets/medical-question-answering-datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd4925a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': \"If you are a doctor, please answer the medical questions based on the patient's description.\", 'input': 'Hey Just wondering.  I am a 39 year old female, pretty smallMy heart rate is around 97 to 106 at rest, and my BP is 140/90 and twice I get 175/118I did visit a doctor because I  didnt feel well past month or twoThen the doctor gave me a heart medicine to take the pulse down and BP  (its still in further examination.)But I wondering what it can be? Do I need the medicine really?  Is that bad ?', 'output': \"hello and thank you for using chatbot. i carefully read your question and i understand your concern. i will try to explain you something and give you my opinion. we talk about hypertension if we have mean value that exceeds 140 / 90 mmhg. a person might have high value during emotional and physicals trees so it's mandatory to judge on mean values. usaly hypertension does not give any symptoms but left untreated he slowly modifies the heart. according to heart rhythm, the normal rate is between 50-100 beat for minute. when it exceeds 100 we talk about sinus tachycardia. this might have different causes to simple emotional stress, physical activity, coffee consumption or pathologies like anemia, hyperthyroidism. so if we diagnose hypertension and rhythm issue we have to find they cause and of course treat them. if you treat the hypertension than you have nothing to worry. if i was your treating doctor i will recommend some examination like an electrocardiogram, a cardiac echo, a full blood analyze, a holder rhythm and pressure monitoring. this gives a better view how to treat the problem, medical or not. but as you catch values up to 170 i think medical treatment is necessary. hope i was helpful. wish you good health. best regards.\", '__index_level_0__': 157271}\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "27046ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instruction', 'input', 'output', '__index_level_0__']\n"
     ]
    }
   ],
   "source": [
    "print(ds[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "09638212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to: datasets/medical_qa/medical_qa.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load your dataset\n",
    "dataset = ds  # or local path if using load_from_disk\n",
    "\n",
    "# Output file\n",
    "output_path = \"datasets/medical_qa/medical_qa.jsonl\"\n",
    "\n",
    "eot = tokenizer.eos_token\n",
    "\n",
    "# Convert each row to the desired format\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in dataset[\"train\"]:\n",
    "        instruction = example[\"instruction\"].strip()\n",
    "        input_text = example[\"input\"].strip()\n",
    "        output_text = example[\"output\"].strip()\n",
    "\n",
    "        chat = {\n",
    "            \"conversations\": [\n",
    "                { \"from\": \"system\", \"value\": instruction },\n",
    "                { \"from\": \"human\", \"value\": input_text },\n",
    "                { \"from\": \"assistant\", \"value\": output_text + eot }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        json.dump(chat, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "68e55af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 246678 examples [00:00, 735946.23 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 246678/246678 [00:00<00:00, 1528070.11 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load from your labeled JSONL files\n",
    "dataset = DatasetDict({\n",
    "    \"train\": load_dataset(\"json\", data_files=\"datasets/medical_qa/medical_qa.jsonl\", split=\"train\"),\n",
    "})\n",
    "\n",
    "# Save it back to the same folder (or a subfolder to avoid overwriting)\n",
    "dataset.save_to_disk(\"datasets/medical_qa/medical_qa_dataset_new\")\n",
    "print(\"Dataset saved to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
