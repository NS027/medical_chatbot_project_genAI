{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "224d71e8",
   "metadata": {},
   "source": [
    "# Fine Tuning use LoRA\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a903571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/TachyHealth/ADA_Dental_Code_to_SBS_V2/data/train-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "139405e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ADA Code                      Description SBS V2.0 Code   \\\n",
      "0      11.0   Comprehensive oral examination    97011-00-00   \n",
      "1      12.0        Periodic oral examination    97012-00-00   \n",
      "2      13.0       Oral examination â€“ limited    97013-00-00   \n",
      "3       NaN                             None    97011-00-10   \n",
      "4       NaN                             None    97011-00-40   \n",
      "\n",
      "                                Short description  \\\n",
      "0                  Comprehensive oral examination   \n",
      "1                              Periodic oral exam   \n",
      "2                              Limited oral exam    \n",
      "3  Oral examination; post operative re-evaluation   \n",
      "4       Oral evaluation; under three years of age   \n",
      "\n",
      "                                    Long Description  Block Comments/Guidance  \n",
      "0                     Comprehensive oral examination    450              None  \n",
      "1                          Periodic oral examination    450              None  \n",
      "2                           Limited oral examination    450              None  \n",
      "3     Oral examination; post operative re-evaluation    450              None  \n",
      "4  Oral evaluation for a patient under three year...    450              None  \n",
      "Index(['ADA Code', 'Description', 'SBS V2.0 Code ', 'Short description',\n",
      "       'Long Description', 'Block', 'Comments/Guidance'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# display the first 5 rows of the dataframe\n",
    "print(df.head())\n",
    "# display the column names of the dataframe\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e7110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-1.5B\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-1.5B\",\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\n",
    "        \"mlp.gate_proj\", \"mlp.up_proj\", \"mlp.down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model = model.to(device)\n",
    "\n",
    "# 3. Load and format dataset\n",
    "dataset = load_dataset(\"TachyHealth/ADA_Dental_Code_to_SBS_V2\", split=\"train\")\n",
    "\n",
    "def format_prompt(example):\n",
    "    if not example.get(\"ADA Code\") or not example.get(\"Short description\") or not example.get(\"Long Description\"):\n",
    "        return None\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "Given a dental procedure code and its short name, explain what it means in detail so a patient can understand.\n",
    "\n",
    "### Code:\n",
    "{int(example['ADA Code'])} - {example['Short description']}\n",
    "\n",
    "### Response:\n",
    "{example['Long Description']}\"\"\"\n",
    "    tokenized = tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": [token if token != tokenizer.pad_token_id else -100 for token in tokenized[\"input_ids\"]]\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(format_prompt, remove_columns=dataset.column_names)\n",
    "tokenized_dataset = tokenized_dataset.filter(lambda x: x and x[\"input_ids\"] is not None)\n",
    "\n",
    "# 4. Collate function\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor([item[\"input_ids\"] for item in batch], dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor([item[\"attention_mask\"] for item in batch], dtype=torch.long),\n",
    "        \"labels\": torch.tensor([item[\"labels\"] for item in batch], dtype=torch.long),\n",
    "    }\n",
    "\n",
    "# 5. DataLoader, optimizer, scheduler\n",
    "train_loader = DataLoader(tokenized_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer, 0, len(train_loader))\n",
    "\n",
    "# 6. Training loop\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for batch in loop:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# 7. Save adapter\n",
    "model.save_pretrained(\"./qwen2.5-lora-dental\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9fa78b",
   "metadata": {},
   "source": [
    "# Fine Tuning for ADA code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a3a5f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "540f2512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Could not load bitsandbytes native library: 'NoneType' object has no attribute 'split'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\shirl\\Desktop\\Dev\\my_finetune_project\\venv_finetune\\Lib\\site-packages\\bitsandbytes\\cextension.py\", line 85, in <module>\n",
      "    lib = get_native_library()\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shirl\\Desktop\\Dev\\my_finetune_project\\venv_finetune\\Lib\\site-packages\\bitsandbytes\\cextension.py\", line 64, in get_native_library\n",
      "    cuda_specs = get_cuda_specs()\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shirl\\Desktop\\Dev\\my_finetune_project\\venv_finetune\\Lib\\site-packages\\bitsandbytes\\cuda_specs.py\", line 39, in get_cuda_specs\n",
      "    cuda_version_string=(get_cuda_version_string()),\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shirl\\Desktop\\Dev\\my_finetune_project\\venv_finetune\\Lib\\site-packages\\bitsandbytes\\cuda_specs.py\", line 29, in get_cuda_version_string\n",
      "    major, minor = get_cuda_version_tuple()\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shirl\\Desktop\\Dev\\my_finetune_project\\venv_finetune\\Lib\\site-packages\\bitsandbytes\\cuda_specs.py\", line 24, in get_cuda_version_tuple\n",
      "    major, minor = map(int, torch.version.cuda.split(\".\"))\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\n",
      "CUDA Setup failed despite CUDA being available. Please run the following command to get more information:\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      "Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n",
      "to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n",
      "and open an issue at: https://github.com/bitsandbytes-foundation/bitsandbytes/issues\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-1.5B\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-1.5B\",\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\n",
    "        \"mlp.gate_proj\", \"mlp.up_proj\", \"mlp.down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c4ba2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1184420d9b604dacb9649d71e4a7de03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/594 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2bd4972d174176be856ce55c71fa9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/341 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Load and format dataset\n",
    "dataset = load_dataset(\"TachyHealth/ADA_Dental_Code_to_SBS_V2\", split=\"train\")\n",
    "\n",
    "def format_prompt(example):\n",
    "    if not example.get(\"ADA Code\") or not example.get(\"Short description\") or not example.get(\"Long Description\"):\n",
    "        return None\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "Given a dental procedure code and its short name, explain what it means in detail so a patient can understand.\n",
    "\n",
    "### Code:\n",
    "{int(example['ADA Code'])} - {example['Short description']}\n",
    "\n",
    "### Response:\n",
    "{example['Long Description']}\"\"\"\n",
    "    tokenized = tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": [token if token != tokenizer.pad_token_id else -100 for token in tokenized[\"input_ids\"]]\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(format_prompt, remove_columns=dataset.column_names)\n",
    "tokenized_dataset = tokenized_dataset.filter(lambda x: x and x[\"input_ids\"] is not None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ebb5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Collate function\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor([item[\"input_ids\"] for item in batch], dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor([item[\"attention_mask\"] for item in batch], dtype=torch.long),\n",
    "        \"labels\": torch.tensor([item[\"labels\"] for item in batch], dtype=torch.long),\n",
    "    }\n",
    "\n",
    "# 5. DataLoader, optimizer, scheduler\n",
    "train_loader = DataLoader(tokenized_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer, 0, len(train_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87bfc5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 341/341 [1:01:58<00:00, 10.90s/it, loss=1]    \n",
      "c:\\Users\\shirl\\Desktop\\Dev\\my_finetune_project\\venv_finetune\\Lib\\site-packages\\peft\\utils\\other.py:588: UserWarning: Unable to fetch remote file due to the following error (ProtocolError('Connection aborted.', ConnectionAbortedError(10053, 'ä½ çš„ä¸»æœºä¸­çš„è½¯ä»¶ä¸­æ­¢äº†ä¸€ä¸ªå·²å»ºç«‹çš„è¿žæŽ¥ã€‚', None, 10053, None)), '(Request ID: 03e2b23d-5a65-4ace-ac00-cf8117d15edd)') - silently ignoring the lookup for the file config.json in Qwen/Qwen2.5-1.5B.\n",
      "  warnings.warn(\n",
      "c:\\Users\\shirl\\Desktop\\Dev\\my_finetune_project\\venv_finetune\\Lib\\site-packages\\peft\\utils\\save_and_load.py:154: UserWarning: Could not find a config file in Qwen/Qwen2.5-1.5B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 6. Training loop\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for batch in loop:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# 7. Save adapter\n",
    "model.save_pretrained(\"./qwen2.5-lora-dental\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
