# -*- coding: utf-8 -*-
"""Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OTkeW0-E9wxiHAekwb_QilVt-d4_630S
"""



"""# Evaluate and compare the response quality of llama-3.2-90b-vision-preview and MMed-LLaMA 3 on the VQA dataset we created"""

!pip install datasets groq bert_score tqdm matplotlib seaborn torch transformers

# Compare LLaMA 3.2 90B vs MMed-LLaMA 3 on Medical VQA Dataset

import os
import base64
from io import BytesIO
from datasets import load_dataset
from groq import Groq
from bert_score import score
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load Dataset
print("Loading dataset...")
dataset = load_dataset("SiyunHE/medical-pilagemma-lora", split="train")

sampled_data = dataset.shuffle(seed=42).select(range(30))

# GROQ API Key
groq_api_key = "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"
client_groq = Groq(api_key=groq_api_key)

# Load MMed-LLaMA 3 model locally
model_id = "Henrychur/MMed-Llama-3-8B"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).cuda().eval()

# Encode PIL Image to base64
def encode_pil_image(pil_image):
    buffered = BytesIO()
    pil_image.save(buffered, format="JPEG")
    return base64.b64encode(buffered.getvalue()).decode("utf-8")

# Generation Functions

def analyze_image_with_llama(query, image):
    encoded_image = encode_pil_image(image)
    messages = [{"role": "user", "content": [
        {"type": "text", "text": query},
        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{encoded_image}"}},
    ]}]

    response = client_groq.chat.completions.create(
        messages=messages,
        model="llama-3.2-90b-vision-preview",
    )
    return response.choices[0].message.content


def analyze_image_with_mmed_llama(query, image):
    prompt = f"[Image] {query}"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=512)

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# Evaluation

def evaluate_model(generate_fn, model_name):
    model_answers = []
    ground_truth_answers = []

    print(f"Generating answers for {model_name}...")

    for example in tqdm(sampled_data):
        image = example['image']
        question = example['question']
        ground_truth = example['answer']

        model_response = generate_fn(question, image)
        model_answers.append(model_response)
        ground_truth_answers.append(ground_truth)

    print("Calculating BERTScore...")
    P, R, F1 = score(model_answers, ground_truth_answers, lang="en")
    print(f"{model_name} Average BERTScore F1: {F1.mean().item()}")

    return F1

# Evaluate both models
f1_llama = evaluate_model(analyze_image_with_llama, "LLaMA 3.2 90B")
f1_mmed_llama = evaluate_model(analyze_image_with_mmed_llama, "MMed-LLaMA 3")

# Plot
plt.figure(figsize=(8,5))
sns.histplot(f1_llama.tolist(), bins=10, kde=True, color="#00BFC4", edgecolor='black', label='LLaMA 3.2 90B')
sns.histplot(f1_mmed_llama.tolist(), bins=10, kde=True, color="#F8766D", edgecolor='black', label='MMed-LLaMA 3')
plt.xlabel('BERTScore F1', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('Comparison of Answer Similarity', fontsize=14, weight='bold')
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.legend()
plt.tight_layout()
plt.show()

"""- LLaMA 3.2 90B Average BERTScore F1: 0.8584278225898743
- MMed-LLaMA 3 Average BERTScore F1: 0.7670438289642334

Here is the meaning of BERTScore F1 <br>
0.90 | Very High Semantic Match | Same phrasing / Paraphrasing <br>
0.85 - 0.90 | Good Semantic Similarity | Answers are meaningful & relevant<br>
0.80 - 0.85 | Acceptable but Room for Improvement | Maybe minor missing info / wording <br>
<0.80 | Weak Similarity | Likely incomplete / off-topic answers

The histogram above compares the BERTScore F1 distributions between LLaMA 3.2 90B Vision-Instruct and MMed-LLaMA 3 on 30 randomly selected samples from our medical VQA dataset. As shown in the figure, LLaMA 3.2 90B demonstrates a significantly higher semantic similarity to the ground truth answers generated by GPT-4o, with an average BERTScore F1 of 0.8584. In contrast, MMed-LLaMA 3 yields a lower average BERTScore F1 of 0.7670, with most of its responses clustering around the 0.75 to 0.78 range. The clear separation between the two distributions indicates that LLaMA 3.2 90B consistently generates responses that are more semantically aligned and closer in meaning to the ground truth answers. This result highlights the superior language generation capability of LLaMA 3.2 90B in this medical VQA task compared to MMed-LLaMA 3, despite both models being evaluated on the same set of images and questions.
"""