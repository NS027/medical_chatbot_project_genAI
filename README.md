# ğŸ©º AI Doctor: Real-Time Medical Assistance with Multimodal Large Language Model

## ğŸ§  Overview

This project introduces a real-time **AI Doctor** chatbot powered by a **Multimodal Large Language Model (LLM)**. It supports both **voice** and **text** input, understands **medical images**, and provides responses via **speech** and **text**. Additional features include summarization and translation â€” offering a natural and accessible healthcare assistant.

---

## ğŸŒŸ Features

- ğŸ” **Multimodal LLM**  
  Handles both text and image inputs for medical understanding  
  _â†ªï¸ Implemented in `brain_of_the_doctor.py`_

- ğŸ—£ï¸ **Speech-to-Text (STT)**  
  Converts patient voice input into text using Whisper  
  _â†ªï¸ Implemented in `voice_of_the_patient.py`_

- ğŸ”ˆ **Text-to-Speech (TTS)**  
  Generates spoken responses from AI-generated answers  
  _â†ªï¸ Implemented in `voice_of_the_doctor.py`_

- ğŸ’¬ **Interactive Gradio UI**  
  Provides a simple, real-time interface for user interaction  
  _â†ªï¸ Implemented in `gradio_app.py`_
---

## ğŸ§ª Our Contributions

### 1. ğŸ§‘â€âš•ï¸ AI Doctor Application
A conversational AI that helps users make informed decisions about seeking medical care â€” potentially reducing unnecessary visits and improving access.
  
<p align="center">
  <img src="UI.png" alt="User Interface" width="80%"/>
</p>

---

### 2. ğŸ—‚ï¸ Custom Dataset Creation

Most existing medical datasets lack paired image-text data and focus on modalities like CT or PET scans â€” not suitable for real-world symptom images. To address this:

- We built a medical VQA dataset using **user-uploaded images** and **GPT-4o-generated** Q&A pairs simulating real patient inquiries and expert-level answers.
ğŸ¤— Dataset available on [Hugging Face](https://huggingface.co/datasets/SiyunHE/medical-pilagemma-lora)
ğŸ“‚ Data creation details: See [`data_creation/`](data_creation)

---

### 3. ğŸ”§ Fine-tuning with LoRA

We fine-tuned **PaliGemma** using **LoRA** on our dataset to build a lightweight alternative to **LLaMA 3.2 11B Vision-Instruct**.

- Applied LoRA to cross-attention layers for efficient adaptation
- Trained with Hugging Face Trainer on **Google Colab A100**
- Learning rate: `5e-5`, Batch size: `4`, Epochs: `3`
- Outperforms base PaliGemma on medical VQA, with a smaller footprint

ğŸ¤— [Fine-tuned model on Hugging Face](https://huggingface.co/SiyunHE/medical-pilagemma-lora)  
ğŸ“ Fine-tuning details: See [`experiments/`](experiments)

---

### 4. ğŸ“Š Model Evaluation

We evaluated model performance using **BERTScore F1** on 30 samples from our dataset, comparing:

- ğŸ”¬ **LLaMA-3.2-11B-Vision-Instruct** (model we used)
- ğŸ§¬ **MMed-LLaMA 3** (trained on medical data)

<p align="center">
  <img src="evaluation/answer_similarity_comparison.png" alt="Evaluation Chart" width="80%">
</p>

ğŸ“ˆ Results: Our model consistently achieves higher semantic alignment with ground truth answers, indicating stronger response quality for real-world medical VQA tasks.

ğŸ“ evaluation details: See [`evaluation/`](evaluation)

---

## âš™ï¸ Setup Instructions

### ğŸ› ï¸ Prerequisites

- ğŸ **Python 3.11+**

- ğŸ”‘ **Environment Variables:**
 ğŸ‘‰ See `.env.example` for required environment variables.

  - `GROQ_API_KEY` â€” _(Free)_  
    Required for **speech-to-text (STT)** using `whisper-large-v3`.

  - `HF_TOKEN` â€” _(Free)_  
    Needed to load the **Google PaliGemma** model: `google/paligemma-3b-pt-224`.

  - `OPENROUTER_API_KEY` â€” _(Paid or Free)_  
    Used to access **meta-llama/llama-3.2-11b-vision-instruct**  
    > We currently use the **paid version** for more stable performance,  
    > but you may switch to the free version:  
    > `meta-llama/llama-3.2-11b-vision-instruct:free`

### ğŸ› ï¸Setup steps

1. Clone this repository:
   ```bash
   git clone medical_chatbot_project_genAI
   cd medical_chatbot_project_genAI
   ```
2. create a new environment with conda (recommend)
   ```bash
   conda create --name ai_doctor python=3.11
   ```
   create a new environment without conde
   ```bash
   python3 -m venv venv
   source venv/bin/activate   # On macOS/Linux
   .\venv\Scripts\activate     # On Windows
   ```
3. activate the enviornment
   ```bash
   conda activate ai_doctor
   ```
4. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
5. start the application
   ```bash
   gradio gradio_app.py
   ```



### ğŸš€ Future Development

- Add real-time medical knowledge via **Retrieval-Augmented Generation (RAG)** to overcome LLM knowledge cutoffs.
- Use **Medical Communication Protocols (MCP)** for better scalability and healthcare system integration.
- Expand **language support** and enhance **medical reasoning** capabilities.
- Conduct **clinical validation** to assess safety and real-world effectiveness.



## ğŸ“šReference:
https://github.com/AIwithhassan/ai-doctor-2.0-voice-and-vision <br>
https://github.com/RyanWangZf/MedCLIP
